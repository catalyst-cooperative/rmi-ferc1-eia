{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FERC <> EIA Granular Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the type of problem we are trying to solve:\n",
    "- A classification problem\n",
    "    - A Multi-Class Classification problem*\n",
    "- A deterministic problem\n",
    "- A record linkage problem\n",
    "\n",
    "Right now, we are using the recordlinkage package. We're using logistic regression classifier because it fits all of the above.\n",
    "\n",
    "To consider:\n",
    "- Maybe we want to run the records with fuel cost data through a different matching model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pudl\n",
    "import pudl.constants as pc\n",
    "import pudl.extract.ferc1\n",
    "import sqlalchemy as sa\n",
    "import logging\n",
    "import sys\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import scipy\n",
    "import statistics\n",
    "import yaml\n",
    "\n",
    "import recordlinkage as rl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from pudl.output.ferc1 import *\n",
    "from pudl_rmi.connect_ferc1_to_eia import *\n",
    "from pudl_rmi.make_plant_parts_eia import *\n",
    "import pudl_rmi.connect_ferc1_to_eia\n",
    "pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "pudl_engine = sa.create_engine(pudl_settings[\"pudl_db\"])\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in data (EIA, FERC and training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_training = pathlib.Path().cwd().parent /'inputs'/'train_ferc1_to_eia.csv'\n",
    "file_path_mul = pathlib.Path().cwd().parent /'outputs' /'master_unit_list.pkl.gz'\n",
    "# pudl output object for ferc data\n",
    "pudl_out = pudl.output.pudltabl.PudlTabl(pudl_engine,freq='AS',fill_fuel_cost=True,roll_fuel_cost=True,fill_net_gen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = InputManager(file_path_training, file_path_mul, pudl_out)\n",
    "features_all = (Features(feature_type='all', inputs=inputs)\n",
    "                .get_features(clobber=False))\n",
    "features_train = (Features(feature_type='training', inputs=inputs)\n",
    "                  .get_features(clobber=False))\n",
    "tuner = ModelTuner(features_train, inputs.get_train_index(), n_splits=10)\n",
    "\n",
    "matcher = MatchManager(best=tuner.get_best_fit_model(), inputs=inputs)\n",
    "matches_best = matcher.get_best_matches(features_train, features_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for connect_deprish_to_ferc1\n",
    "file_path_steam_ferc1 = pathlib.Path().cwd().parent / 'outputs' /'steam_ferc1.pkl.gz'\n",
    "inputs.steam_df.to_pickle(file_path_steam_ferc1, compression='gzip')\n",
    "\n",
    "file_path_ferc1_eia = pathlib.Path().cwd().parent / 'outputs'/ 'ferc1_to_eia.pkl.gz'\n",
    "matches_best.to_pickle(file_path_ferc1_eia, compression='gzip')\n",
    "file_path_ferc1_eia_csv = pathlib.Path().cwd().parent / 'outputs'/ 'ferc1_to_eia.csv.gz'\n",
    "matches_best.to_csv(file_path_ferc1_eia_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Comparison Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Classificaiton Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame(\n",
    "    data={'feature': features_all.columns,\n",
    "          'weight': matcher.coefs\n",
    "         })\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_hist(all_df,results_df,murk_df, matches_best_df, range):\n",
    "    if all_df is not None:\n",
    "        plt.hist(all_df['score'], \n",
    "                 range=range,\n",
    "                 bins=100,\n",
    "                 color=\"pink\",\n",
    "                 label='all options'\n",
    "                )\n",
    "    if results_df is not None:\n",
    "        plt.hist(results_df['score'], \n",
    "                 range=range,\n",
    "                 bins=100,\n",
    "                 color=\"purple\",\n",
    "                 label='all model matches'\n",
    "                )\n",
    "    if matches_best_df is not None:\n",
    "        plt.hist(matches_best_df['score'], \n",
    "                 range=range,\n",
    "                 bins=100,\n",
    "                 color=\"turquoise\",\n",
    "                 label='winning options'\n",
    "                )\n",
    "    if murk_df is not None:\n",
    "        plt.hist(murk_df['score'], \n",
    "                 range=range,\n",
    "                 bins=100,\n",
    "                 color=\"grey\",\n",
    "                 label='murky wins'\n",
    "                )\n",
    "\n",
    "    plt.title(f'weighted score of comparision features')\n",
    "    plt.xlabel('weighted sum')\n",
    "    plt.ylabel(None)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_hist(matcher.calc_match_stats(features_all),\n",
    "              matcher.matches_model,\n",
    "              matcher.murk_df,\n",
    "              matches_best,\n",
    "              range=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = (matches_best.reset_index().merge(inputs.plant_parts_df.\n",
    "              reset_index()[['record_id_eia','plant_part','capacity_mw']],\n",
    "              on=['record_id_eia'],\n",
    "              how='left',\n",
    "              suffixes=('_feature','')\n",
    "             ).\n",
    "        groupby(['plant_part']).\n",
    "        agg({'capacity_mw':sum,\n",
    "             'score': 'count'}).\n",
    "        assign(count_w=lambda x: x.capacity_mw * x.score,\n",
    "               percent_w=lambda x: x.count_w/x.sum()['count_w'],\n",
    "               percent=lambda x: x.score/x.sum()['score'],\n",
    "              )\n",
    "       )\n",
    "\n",
    "wins.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
